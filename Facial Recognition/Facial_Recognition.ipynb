{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "079499c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a940f0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b74a3f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images belonging to 3 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Jaden': 0, 'Slayde': 1, 'aston': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Processing the training set\n",
    "train_datagen = ImageDataGenerator(rescale= 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip = True)\n",
    "train_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                             target_size = (64,64),\n",
    "                                             batch_size = 2,\n",
    "                                             class_mode = 'categorical')\n",
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e27e5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#Processing the Test Set\n",
    "test_datagen = ImageDataGenerator(rescale= 1./255)\n",
    "test_set = train_datagen.flow_from_directory('dataset/test_set',\n",
    "                                             target_size = (64,64),\n",
    "                                             batch_size = 3,\n",
    "                                             class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f0e0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the CNN Model\n",
    "\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "#convolution layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=18, kernel_size = 3, activation = 'relu', input_shape = [64,64,3]))\n",
    "\n",
    "#Pooling Layer\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2 , strides = 2))\n",
    "\n",
    "#Adding second convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=18, kernel_size = 3, activation = 'relu', input_shape = [64,64,3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2 , strides = 2))\n",
    "\n",
    "#Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units = 64 , activation = 'relu'))\n",
    "\n",
    "#Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units = 3 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3155fcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 63ms/step - loss: 1.4854 - accuracy: 0.4444 - val_loss: 1.3105 - val_accuracy: 0.6000\n",
      "Epoch 2/25\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 1.0899 - accuracy: 0.5556 - val_loss: 0.9040 - val_accuracy: 0.6000\n",
      "Epoch 3/25\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.7498 - accuracy: 0.7778 - val_loss: 1.0127 - val_accuracy: 0.7000\n",
      "Epoch 4/25\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.6831 - accuracy: 1.0000 - val_loss: 0.9736 - val_accuracy: 0.7000\n",
      "Epoch 5/25\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.6727 - accuracy: 0.8889 - val_loss: 0.7174 - val_accuracy: 0.7000\n",
      "Epoch 6/25\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5330 - accuracy: 0.8889 - val_loss: 0.8550 - val_accuracy: 0.7000\n",
      "Epoch 7/25\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3708 - accuracy: 1.0000 - val_loss: 0.7629 - val_accuracy: 0.8000\n",
      "Epoch 8/25\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.2378 - accuracy: 1.0000 - val_loss: 0.7836 - val_accuracy: 0.8000\n",
      "Epoch 9/25\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.1269 - accuracy: 1.0000 - val_loss: 0.9442 - val_accuracy: 0.8000\n",
      "Epoch 10/25\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0683 - accuracy: 1.0000 - val_loss: 0.9194 - val_accuracy: 0.8000\n",
      "Epoch 11/25\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 1.0594 - val_accuracy: 0.8000\n",
      "Epoch 12/25\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 1.7089 - val_accuracy: 0.6000\n",
      "Epoch 13/25\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 2.3864 - val_accuracy: 0.2000\n",
      "Epoch 14/25\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 1.6310 - val_accuracy: 0.7000\n",
      "Epoch 15/25\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.6978 - val_accuracy: 0.8000\n",
      "Epoch 16/25\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.4881 - val_accuracy: 0.8000\n",
      "Epoch 17/25\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.7083 - val_accuracy: 0.7000\n",
      "Epoch 18/25\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.7866 - val_accuracy: 0.7000\n",
      "Epoch 19/25\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.7374 - val_accuracy: 0.7000\n",
      "Epoch 20/25\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 7.8692e-04 - accuracy: 1.0000 - val_loss: 1.7070 - val_accuracy: 0.7000\n",
      "Epoch 21/25\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 7.4301e-04 - accuracy: 1.0000 - val_loss: 1.6906 - val_accuracy: 0.7000\n",
      "Epoch 22/25\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.3589e-04 - accuracy: 1.0000 - val_loss: 1.7732 - val_accuracy: 0.7000\n",
      "Epoch 23/25\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.4856e-04 - accuracy: 1.0000 - val_loss: 2.0801 - val_accuracy: 0.7000\n",
      "Epoch 24/25\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 5.0872e-04 - accuracy: 1.0000 - val_loss: 1.7342 - val_accuracy: 0.7000\n",
      "Epoch 25/25\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 4.3210e-04 - accuracy: 1.0000 - val_loss: 1.9680 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c30479d1d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the CNN Model\n",
    "\n",
    "#compiling the cnn\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "\n",
    "#training the cnn on training set and evaluating it on the test set\n",
    "cnn.fit(x = train_set , validation_data = test_set , epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbc583d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted label: Jaden\n"
     ]
    }
   ],
   "source": [
    "#making single predictions\n",
    "test_image = tf.keras.utils.load_img('dataset/test_set/aston/image_1.jpg', target_size=(64,64))\n",
    "test_image = tf.keras.utils.img_to_array(test_image)\n",
    "test_image = test_image/255\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "result_index = np.argmax(result)\n",
    "train_index = train_set.class_indices\n",
    "predicted_label = list(train_index.keys())[list(train_index.values()).index(result_index)]\n",
    "print(\"Predicted label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0281a95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAyCAYAAADBYth2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAMpklEQVR4nO3de0xb5RsH8G8p17VQsCuXyAbINpRNsgzjlMFkg62OAv/olgIq4C7EDRiaxRj/cExR5jIXkEsTEu0iziwyoiQILkOZbrosW4AoUwgywF2UywZ4gcGA9/fH0v52KNddYLXfT0LCec9zznnet4U+Pec9rUwIIUBEREQ2y26+EyAiIqL5xWKAiIjIxrEYICIisnEsBoiIiGwciwEiIiIbx2KAiIjIxrEYICIisnEsBoiIiGwciwEiIiIbx2KAaAb8/f2RkpJiXj558iRkMhlOnjw5bzmNNz7HychkMmRnZ896/4cPH4ZMJsP58+dnn9wksrOzIZPJ7tn+7oU7HR8ia8ZigB54phch04+zszOWLVuG9PR0dHZ2znd6s1JVVcUXmnvI398fsbGx850GkdWzn+8EiGbq7bffRkBAAG7cuIHTp0/DYDCgqqoKjY2NWLBgwZzmsnbtWgwODsLR0XFW21VVVaGoqIgFARE9UFgMkNXYtGkTnnjiCQDAtm3boFarcejQIVRUVCAhIWHCbf79918oFIp7noudnR2cnZ3v+X6JiOYDLxOQ1Vq/fj0AoK2tDQCQkpICpVKJ1tZWxMTEwNXVFUlJSQCAsbEx5OXlYfny5XB2doaXlxfS0tLQ29sr2acQAjk5OfD19cWCBQuwbt06XLhwweLYk80ZOHv2LGJiYuDh4QGFQoGQkBDk5+eb8ysqKgIAyWUPk3ud40x1dHRg586dCAoKgouLC9RqNTZv3oz29vYJ4wcGBpCWlga1Wg03Nze89NJLFjkCQHV1NSIiIqBQKODq6gqdTjejPHt6etDU1ISBgYE76s/BgwcRFhYGtVoNFxcXhIaG4tixYxZxQ0NDePXVV6HRaODq6or4+Hhcvnx5wn1euXIFL7/8Mry8vODk5ITly5fj448/lsSYnhOff/453n33Xfj6+sLZ2RlRUVH47bff7qgvRHOFZwbIarW2tgIA1Gq1uW1kZARarRbh4eE4ePCg+fJBWloaDh8+jNTUVGRmZqKtrQ2FhYWor6/HDz/8AAcHBwDAW2+9hZycHMTExCAmJgZ1dXXYuHEjhoeHp83nxIkTiI2NhY+PD3bv3g1vb2/8+uuvqKysxO7du5GWloarV6/ixIkTKC0ttdh+LnKcyLlz5/Djjz9Cr9fD19cX7e3tMBgMiIyMxC+//GJxCSY9PR3u7u7Izs5Gc3MzDAYDOjo6zC+GAFBaWork5GRotVq8//77GBgYgMFgQHh4OOrr6+Hv7z9pPoWFhdi3bx9qa2sRGRk56/7k5+cjPj4eSUlJGB4extGjR7F582ZUVlZCp9OZ47Zt24ZPP/0UiYmJCAsLw7fffitZb9LZ2YmnnnoKMpkM6enp0Gg0qK6uxtatW/HXX38hKytLEr9//37Y2dlhz5496O/vx4EDB5CUlISzZ8/Oui9Ec0YQPeCMRqMAIGpqakR3d7e4dOmSOHr0qFCr1cLFxUVcvnxZCCFEcnKyACDeeOMNyfanTp0SAMSRI0ck7V9//bWkvaurSzg6OgqdTifGxsbMcW+++aYAIJKTk81ttbW1AoCora0VQggxMjIiAgIChJ+fn+jt7ZUc5/Z97dq1S0z0Z3c/cpwMALF3717z8sDAgEXMmTNnBADxySefmNtMj0NoaKgYHh42tx84cEAAEBUVFUIIIf7++2/h7u4utm/fLtnnn3/+KVQqlaR97969FuNhajON7VT8/PyETqeTtI3vz/DwsFixYoVYv369ua2hoUEAEDt37pTEJiYmWozP1q1bhY+Pj+jp6ZHE6vV6oVKpzMczPScee+wxMTQ0ZI7Lz88XAMTPP/88bX+I5gsvE5DViI6OhkajwaJFi6DX66FUKvHFF1/g4YcflsS98sorkuWysjKoVCps2LABPT095p/Q0FAolUrU1tYCAGpqajA8PIyMjAzJ6fvx7/wmUl9fj7a2NmRlZcHd3V2ybia3zs1FjpNxcXEx/37z5k1cu3YNS5Ysgbu7O+rq6izid+zYYT5LAdwab3t7e1RVVQG4dYakr68PCQkJkr7I5XKsXr3a3JfJZGdnQwhxR2cFxvent7cX/f39iIiIkPTFlGtmZqZk2/HjKIRAeXk54uLiIISQ9Eer1aK/v99ijFJTUyUTSyMiIgAAFy9evKP+EM0FXiYgq1FUVIRly5bB3t4eXl5eCAoKgp2dtJ61t7eHr6+vpK2lpQX9/f3w9PSccL9dXV0Abl07B4ClS5dK1ms0Gnh4eEyZm+mSxYoVK2beoTnOcTKDg4PIzc2F0WjElStXIIQwr+vv77eIH39spVIJHx8f8xyDlpYWAP+f0zGem5vbHeU5U5WVlcjJyUFDQwOGhobM7bcXTx0dHbCzs0NgYKBk26CgIMlyd3c3+vr6UFJSgpKSkgmPZ3psTBYvXixZNj0uE82rIHpQsBggq/Hkk0+a7yaYjJOTk0WBMDY2Bk9PTxw5cmTCbTQazT3L8U7NZ44ZGRkwGo3IysrC008/DZVKBZlMBr1ej7GxsVnvz7RNaWkpvL29Ldbb29+/fzunTp1CfHw81q5di+LiYvj4+MDBwQFGoxGfffbZrPdn6ssLL7yA5OTkCWNCQkIky3K5fMK424ssogcNiwH6zwsMDERNTQ3WrFkjOYU8np+fH4Bb72wfeeQRc3t3d/e07+pM7zAbGxsRHR09adxklwzmIsfJHDt2DMnJyfjggw/MbTdu3EBfX9+E8S0tLVi3bp15+Z9//sEff/yBmJgYc18AwNPTc8qxuB/Ky8vh7OyM48ePw8nJydxuNBolcX5+fhgbG0Nra6vkbEBzc7MkznSnwejo6Jz3hWgucc4A/edt2bIFo6OjeOeddyzWjYyMmF/0oqOj4eDggIKCAsm7uLy8vGmPsWrVKgQEBCAvL8/iRfT2fZk+82B8zFzkOBm5XG7xrrWgoACjo6MTxpeUlODmzZvmZYPBgJGREWzatAkAoNVq4ebmhvfee08SZ9Ld3T1lPndza6FcLodMJpPk3t7eji+//FISZ8r1ww8/lLSPH0e5XI7nnnsO5eXlaGxstDjedH0hshY8M0D/ec888wzS0tKQm5uLhoYGbNy4EQ4ODmhpaUFZWRny8/Px/PPPQ6PRYM+ePcjNzUVsbCxiYmJQX1+P6upqLFy4cMpj2NnZwWAwIC4uDitXrkRqaip8fHzQ1NSECxcu4Pjx4wCA0NBQALcmrmm1Wsjlcuj1+jnJcTKxsbEoLS2FSqVCcHAwzpw5g5qaGsktm7cbHh5GVFQUtmzZgubmZhQXFyM8PBzx8fEAbs0JMBgMePHFF7Fq1Sro9XpoNBr8/vvv+Oqrr7BmzRoUFhZOms/d3Fqo0+lw6NAhPPvss0hMTERXVxeKioqwZMkS/PTTT+a4lStXIiEhAcXFxejv70dYWBi++eabCT8PYP/+/aitrcXq1auxfft2BAcH4/r166irq0NNTQ2uX78+qxyJHkjzdh8D0QyZbmk7d+7clHHJyclCoVBMur6kpESEhoYKFxcX4erqKh5//HHx+uuvi6tXr5pjRkdHxb59+4SPj49wcXERkZGRorGxUfj5+U15a6HJ6dOnxYYNG4Srq6tQKBQiJCREFBQUmNePjIyIjIwModFohEwms7it7l7mOBmMu3Wut7dXpKamioULFwqlUim0Wq1oamqy2J/pcfjuu+/Ejh07hIeHh1AqlSIpKUlcu3bN4ji1tbVCq9UKlUolnJ2dRWBgoEhJSRHnz583x9ztrYWLFy8W8fHxkraPPvpILF26VDg5OYlHH31UGI3GCY8zODgoMjMzhVqtFgqFQsTFxYlLly5ZjI8QQnR2dopdu3aJRYsWCQcHB+Ht7S2ioqJESUmJpL8ARFlZmWTbtrY2AUAYjcZp+0M0X2RCcFYLEVmnhx56CDqdbsIPcSKimeOcASKySq2trejt7UVwcPB8p0Jk9ThngIisysWLF1FVVQWDwQBHR0fo9fr5TonI6vHMABFZle+//x6vvfYaHB0dUVFRgYCAgPlOicjqcc4AERGRjeOZASIiIhvHYoCIiMjGsRggIiKycTO/m2Car2GVTTXzYLpvcJ1u1sKUO5/JAe7cfT3y/HXrgfZAj/ndPM+ndTfJ3ecn033st7X+Gczvvy1rHbX5fpZb77hNaZpuiWn7zTMDRERENo/FABERkY1jMUBERGTjWAwQERHZOBYDRERENo7FABERkY1jMUBERGTj+N0ERERENo5nBoiIiGwciwEiIiIbx2KAiIjIxrEYICIisnEsBoiIiGwciwEiIiIbx2KAiIjIxrEYICIisnEsBoiIiGzc/wCVF3Dm/MlPQAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the image with the predicted label\n",
    "plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Predicted label: \" + predicted_label)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file saved_model already exists.\n",
      "Error occurred while processing: saved_model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('train_indices.json', 'w') as f:\n",
    "    json.dump(train_set.class_indices, f)\n",
    "\n",
    "\n",
    "!mkdir -p saved_model\n",
    "cnn.save('saved_model/my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
